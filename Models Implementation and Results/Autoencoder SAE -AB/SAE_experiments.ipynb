{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"SAE_experiments.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IZTbeNP0Qm73","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oK46hgGitlU","colab_type":"code","colab":{}},"source":["!pip install fiona\n","!pip install rasterio\n","!pip install catboost\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7o3VEujZiQZR","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Weak learners/programming/Artur')\n","\n","import fiona\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import rasterio\n","import scipy\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as torch_data\n","import xgboost as xgb\n","\n","from catboost import CatBoostClassifier\n","from extractor_helper import extractor\n","from imblearn.over_sampling import RandomOverSampler\n","from rasterio.mask import mask\n","from shapely import geometry\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n","from tqdm import tqdm_notebook\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTJi0TVriQbr","colab_type":"code","colab":{}},"source":["#import unlabeled data\n","sat_image = rasterio.open(\"/content/drive/My Drive/Weak learners/programming/Artur/project/1030010056130F00_MS_Pan_modified_KNN.tif\")\n","points1 = fiona.open(\"/content/drive/My Drive/Weak learners/programming/Artur/project/Points_MS_Pan_KNN.shp\", \"r\")\n","points2 = fiona.open(\"/content/drive/My Drive/Weak learners/programming/Artur/project/AB_points.shp\", \"r\")\n","\n","#import labeled data\n","test2_img = rasterio.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test2/pp_2_sat_modified.tif\")\n","test2_points = fiona.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test2/points_2_modified_Copy.shp\", \"r\")\n","\n","test3_img = rasterio.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test3/pp_3_sat_modified.tif\")\n","test3_points = fiona.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test3/targets_Copy.shp\", \"r\")\n","\n","test4_img = rasterio.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test4/pp_4_sat_modified_spline.tif\")\n","test4_points = fiona.open(\"/content/drive/My Drive/Weak learners/programming/Artur/Eval/test4/modified_points_Copy.shp\", \"r\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwfYSdp6bWP4","colab_type":"code","colab":{}},"source":["class AutoEncoder(nn.Module):\n","      def __init__(self,encoder,decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","    \n","      def forward(self,x):\n","        z = self.encoder(x)\n","        out = self.decoder(z)\n","        return out\n","\n","def train(epochs, net, criterion, optimizer,\n","          train_loader, val_loader, scheduler=None,\n","          verbose=True, save_dir=None):\n","  loss_tr =0\n","  loss_val =0\n","  net.to(device)\n","  for epoch in range(1,epochs+1):\n","      net.train()\n","      for X in train_loader:\n","          X = X.to(device)\n","          pred = net(X)\n","          loss = criterion(pred,X)\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","    \n","      net.eval()\n","      for X in val_loader:\n","          X = X.to(device)\n","          val_pred = net(X)\n","          val_loss = criterion(val_pred,X)\n","      loss /= len(train_loader)\n","      val_loss /= len(val_loader)\n","      loss_tr +=loss\n","      loss_val +=val_loss\n","\n","  \n","      if scheduler is not None:\n","          schduler.step()\n","    \n","      freq = max(epochs//20,1)\n","      #if verbose and epoch%freq==0:\n","        #print('Epoch {}/{} || Loss:  Train {:.6f} | Validation {:.6f}'.format(epoch, epochs, loss.item(), val_loss.item())) \n","  return (loss_tr/epochs).item(), (loss_val/epochs).item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lB9aVid0VI3m","colab_type":"text"},"source":["# **Experiment without using coordinates**"]},{"cell_type":"code","metadata":{"id":"93sAflkgEC8O","colab_type":"code","colab":{}},"source":["#Extract data and define image size\n","\n","mse_train=np.zeros((len(range(40,130,20)),len(range(5,16,2))))\n","mse_test=np.zeros((len(range(40,130,20)),len(range(5,16,2))))\n","\n","accuracy_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","accuracy_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","f1_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","f1_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","recall_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","recall_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","precision_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","precision_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","\n","#for s in range(5,16,2):\n","for s in range(5,8,2):\n","  j=int((s-1)/2-2)\n","  # Extract images of the needed size: (2*s)x(2*s)\n","  # unlabeled data used for autoencoder/NN\n","  sm_images1, points1_new,_ = extractor(sat_image,points1,size=s,normalize=True);\n","  sm_images2, points2_new,_ = extractor(sat_image,points2,size=s,normalize=True);\n","  x=len(sm_images1+sm_images2)\n","  images_unlabeled = torch.tensor(np.array(sm_images1+sm_images2),dtype=torch.float32).reshape(x,-1).to(device)\n","  x_train, x_test, y_train, y_test = train_test_split(images_unlabeled, images_unlabeled,test_size=.20, shuffle=True,random_state=12)\n","\n","  # labeled data used for classifiers\n","  patch2,coordinates2,labels2 = extractor(test2_img,test2_points,size=s,normalize=True,labeling=True)\n","  patch3,coordinates3,labels3 = extractor(test3_img,test3_points,size=s,normalize=True,labeling=True)\n","  patch4,coordinates4,labels4 = extractor(test4_img,test4_points,size=s,normalize=True,labeling=True)\n","  y=len(patch2+patch3+patch4)\n","\n","  images_labeled = np.array(patch2+patch3+patch4).reshape(y,-1)\n","  labels = np.array(labels2+labels3+labels4).reshape(y,-1)\n","  x_label_train, x_label_test, y_label_train, y_label_test = train_test_split(images_labeled, labels,test_size=.20, shuffle=True, random_state=12, stratify=labels)\n","  #y_label_test = y_label_test.cpu().detach().numpy()\n","\n","  # Upsample train data \n","  #x_label_train = x_label_train.cpu().detach().numpy()\n","  #y_label_train = y_label_train.cpu().detach().numpy()\n","  ros = RandomOverSampler(random_state=12)\n","  x_label_train, y_label_train = ros.fit_resample(x_label_train, y_label_train)\n","\n","  #for b in range(40,130,20):\n","  for b in range(60,90,20):\n","    i=int(b/20-2)\n","\n","    # The size of the hiddenlayer between input and bottleneck should be in the middle between them\n","    m=int(round(8*((2*s)**2)-(8*((2*s)**2)-b)/2))\n","\n","    # Create NN \n","    # input_size -> m -> b -> m -> output_size,\n","    # where input_size = output_size = 8*(2*s)^2\n","    encoder = nn.Sequential(\n","      \n","      # Between input and hidden layer between input and bottleneck\n","      nn.Linear(8*(2*s)**2,m),\n","      nn.BatchNorm1d(m),\n","      nn.ReLU(),\n","\n","      # Between fist hiddenlayer and bottleneck\n","      nn.Linear(m,b),\n","      nn.BatchNorm1d(b),\n","      nn.ReLU()\n","    )\n","\n","    decoder = nn.Sequential(\n","      \n","      # Between Bottleneck and second hiddenlayer\n","      nn.Linear(b,m),\n","      nn.BatchNorm1d(m),\n","      nn.ReLU(),\n","\n","      #Between second hidden layer and output\n","      nn.Linear(m,8*((2*s)**2)),\n","      nn.Sigmoid(),\n","    )\n","\n","    # Create optimazation parameters of NN\n","    NN_net = AutoEncoder(encoder, decoder)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(NN_net.parameters())\n","    train_loader = torch_data.DataLoader(x_train,batch_size=300, shuffle=True)\n","    val_loader = torch_data.DataLoader(x_test,batch_size=300,shuffle=True)\n","    scheduler=None\n","\n","    # Use 100 epochs\n","    lsstr, lssts = train(100, NN_net, criterion, optimizer, train_loader, val_loader, scheduler)\n","    #print(lsstr, lssts)\n","    \n","    # Compute the reconstructed pictures of our NN\n","    NN_net.eval()\n","    y_train_pred = NN_net(x_train).cpu().detach().numpy()\n","    y_test_pred = NN_net(x_test).cpu().detach().numpy()\n","\n","    # Compute the MSE between original images and the reconstructed output of the autoencoder\n","    mse_train[i,j]=mean_squared_error(y_train.cpu().detach().numpy(),y_train_pred)\n","    mse_test[i,j]=mean_squared_error(y_test.cpu().detach().numpy(),y_test_pred)\n","    \n","    # Save the MSE values for different bottleneck and image sizes\n","    np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/mse_train.csv', mse_train, delimiter=',')\n","    np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/mse_test.csv', mse_test, delimiter=',')\n","\n","    # Compress train and test data by our NN\n","    x_label_train_encode = NN_net.encoder(torch.tensor(x_label_train,dtype=torch.float32).to(device)).cpu().detach().numpy()\n","    x_label_test_encode = NN_net.encoder(torch.tensor(x_label_test,dtype=torch.float32).to(device)).cpu().detach().numpy()\n","\n","    # Initialize and train classifiers we want to test\n","    clf1 = AdaBoostClassifier(random_state=12)\n","    clf2 = xgb.XGBClassifier(n_jobs=-1, random_state=12)\n","    clf3 = CatBoostClassifier(verbose=False,random_state=12)\n","    clf4 = RandomForestClassifier(n_jobs=-1, random_state=12)\n","    clf5 = SVC(random_state=12)\n","    clf6 = LogisticRegression(n_jobs=-1,random_state=12)\n","\n","    #Parameters we optimize our classifiers on\n","    parameters = [{\"n_estimators\":[20,30,50,100,150,200]},\n","              {\"n_estimators\":[50,100,150,200,300,400,500], \"max_depth\":[3,5,6,10]},\n","              {},\n","              {\"n_estimators\":[50,100,150,200,300,400,500], \"max_depth\":[3,5,6,10,15,20]},\n","              {'kernel':['rbf','poly','sigmoid'],'gamma':['scale','auto'],'decision_function_shape':['ovo', 'ovr']},\n","              {}]\n","\n","    # Compute different measures on train and test set for every classifier\n","    clfs=[clf1,clf2,clf3,clf4,clf5,clf6]\n","    for m in range(0,6):\n","      clf_temp = GridSearchCV(clfs[m], param_grid=parameters[m], cv=5, n_jobs=-1, scoring='f1_macro',).fit(x_label_train_encode,y_label_train)\n","      pred_train = clf_temp.predict(x_label_train_encode)\n","      pred_test = clf_temp.predict(x_label_test_encode)\n","      accuracy_train[i,j,m]=accuracy_score(y_label_train,pred_train)\n","      accuracy_test[i,j,m]=accuracy_score(y_label_test,pred_test)\n","      f1_train[i,j,m]=f1_score(y_label_train,pred_train,average='macro')\n","      f1_test[i,j,m]=f1_score(y_label_test,pred_test,average='macro')\n","      recall_train[i,j,m]=recall_score(y_label_train,pred_train,average='macro')\n","      recall_test[i,j,m]=recall_score(y_label_test,pred_test,average='macro')\n","      precision_train[i,j,m]=precision_score(y_label_train,pred_train,average='macro')\n","      precision_test[i,j,m]=precision_score(y_label_test,pred_test,average='macro')\n","\n","    # Save all results\n","    clfs_names=['adaboost','xgb','catboost','rfc','svc','lr']\n","    for m in range(0,6):\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/accuracy_train_'+clfs_names[m]+'.csv', accuracy_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/accuracy_test_'+clfs_names[m]+'.csv', accuracy_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/f1_train_'+clfs_names[m]+'.csv', f1_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/f1_test_'+clfs_names[m]+'.csv', f1_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/recall_train_'+clfs_names[m]+'.csv', recall_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/recall_test_'+clfs_names[m]+'.csv', recall_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/precision_train_'+clfs_names[m]+'.csv', precision_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/Wo_coordinates/precision_test_'+clfs_names[m]+'.csv', precision_test[:,:,m], delimiter=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vE40CFnyU8QA","colab_type":"text"},"source":["# **Experiment using coordinates**"]},{"cell_type":"code","metadata":{"id":"Y91_uBvTI1b9","colab_type":"code","colab":{}},"source":["#Extract data and define image size\n","\n","mse_train=np.zeros((len(range(40,130,20)),len(range(5,16,2))))\n","mse_test=np.zeros((len(range(40,130,20)),len(range(5,16,2))))\n","\n","accuracy_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","accuracy_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","f1_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","f1_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","recall_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","recall_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","precision_train=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","precision_test=np.zeros((len(range(40,130,20)),len(range(5,16,2)),6))\n","\n","for s in range(5,16,2):\n","  j=int((s-1)/2-2)\n","  # Extract images of the needed size: (2*s)x(2*s)\n","  # unlabeled data used for autoencoder/NN\n","  sm_images1, points1_new,_ = extractor(sat_image,points1,size=s,normalize=True);\n","  sm_images2, points2_new,_ = extractor(sat_image,points2,size=s,normalize=True);\n","  x=len(sm_images1+sm_images2)\n","\n","  images_unlabeled = torch.tensor(np.array(sm_images1+sm_images2),dtype=torch.float32).reshape(x,-1).to(device)\n","  co = torch.tensor(np.array(points1_new+points2_new),dtype=torch.float32).to(device)\n","  co[:,0]=co[:,0]/torch.max(co[:,0])\n","  co[:,1]=co[:,1]/torch.max(co[:,1])\n","  images_unlabeled_co = torch.cat((images_unlabeled,co),1)\n","  x_train, x_test, y_train, y_test = train_test_split(images_unlabeled_co, images_unlabeled_co,test_size=.20, shuffle=True,random_state=12)\n","\n","  # labeled data used for classifiers\n","  patch2,coordinates2,labels2 = extractor(test2_img,test2_points,size=s,normalize=True,labeling=True)\n","  patch3,coordinates3,labels3 = extractor(test3_img,test3_points,size=s,normalize=True,labeling=True)\n","  patch4,coordinates4,labels4 = extractor(test4_img,test4_points,size=s,normalize=True,labeling=True)\n","  y=len(patch2+patch3+patch4)\n","\n","  images_labeled = np.array(patch2+patch3+patch4).reshape(y,-1)\n","  co = np.array(coordinates2+coordinates3+coordinates4)\n","  co[:,0]=co[:,0]/np.max(co[:,0])\n","  co[:,1]=co[:,1]/np.max(co[:,1])\n","  images_labeled_co = np.concatenate((images_labeled,co),axis=1)\n","\n","  labels = np.array(labels2+labels3+labels4).reshape(y,-1)\n","  x_label_train, x_label_test, y_label_train, y_label_test = train_test_split(images_labeled_co, labels,test_size=.20, shuffle=True, random_state=12, stratify=labels)\n","\n","  # Upsample train data \n","  ros = RandomOverSampler(random_state=12)\n","  x_label_train, y_label_train = ros.fit_resample(x_label_train, y_label_train)\n","\n","\n","  for b in range(40,130,20):    \n","    i=int(b/20-2)\n","    # The size of the hiddenlayer between input and bottleneck should be in the middle between them\n","    m=int(round(8*((2*s)**2)-(8*((2*s)**2)-b)/2))\n","\n","    # Create NN \n","    # input_size -> m -> b -> m -> output_size,\n","    # where input_size = output_size = 8*(2*s)^2\n","    encoder = nn.Sequential(\n","      \n","      # Between input and hidden layer between input and bottleneck\n","      nn.Linear(2+8*(2*s)**2,m),\n","      nn.BatchNorm1d(m),\n","      nn.ReLU(),\n","\n","      # Between fist hiddenlayer and bottleneck\n","      nn.Linear(m,b),\n","      nn.BatchNorm1d(b),\n","      nn.ReLU()\n","    )\n","\n","    decoder = nn.Sequential(\n","      \n","      # Between Bottleneck and second hiddenlayer\n","      nn.Linear(b,m),\n","      nn.BatchNorm1d(m),\n","      nn.ReLU(),\n","\n","      #Between second hidden layer and output\n","      nn.Linear(m,2+8*((2*s)**2)),\n","      nn.Sigmoid(),\n","    )\n","\n","    # Create optimazation parameters of NN\n","    NN_net = AutoEncoder(encoder, decoder)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(NN_net.parameters())\n","    train_loader = torch_data.DataLoader(x_train,batch_size=300, shuffle=True)\n","    val_loader = torch_data.DataLoader(x_test,batch_size=300,shuffle=True)\n","    scheduler=None\n","\n","    # Use 100 epochs\n","    lsstr, lssts = train(100, NN_net, criterion, optimizer, train_loader, val_loader, scheduler)\n","    #print(lsstr, lssts)\n","    \n","    # Compute the reconstructed pictures of our NN\n","    NN_net.eval()\n","    y_train_pred = NN_net(x_train).cpu().detach().numpy()\n","    y_test_pred = NN_net(x_test).cpu().detach().numpy()\n","\n","    # Compute the MSE between original images and the reconstructed output of the autoencoder\n","    mse_train[i,j]=mean_squared_error(y_train.cpu().detach().numpy(),y_train_pred)\n","    mse_test[i,j]=mean_squared_error(y_test.cpu().detach().numpy(),y_test_pred)\n","    \n","    # Save the MSE values for different bottleneck and image sizes\n","    np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/mse_train.csv', mse_train, delimiter=',')\n","    np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/mse_test.csv', mse_test, delimiter=',')\n","\n","    # Compress train and test data by our NN\n","    x_label_train_pred = NN_net.encoder(torch.tensor(x_label_train,dtype=torch.float32).to(device)).cpu().detach().numpy()\n","    x_label_test_pred = NN_net.encoder(torch.tensor(x_label_test,dtype=torch.float32).to(device)).cpu().detach().numpy()\n","\n","    # Initialize and train classifiers we want to test\n","    clf1 = AdaBoostClassifier(random_state=12)\n","    clf2 = xgb.XGBClassifier(n_jobs=-1, random_state=12)\n","    clf3 = CatBoostClassifier(verbose=False,random_state=12)\n","    clf4 = RandomForestClassifier(n_jobs=-1, random_state=12)\n","    clf5 = SVC(random_state=12)\n","    clf6 = LogisticRegression(n_jobs=-1,random_state=12)\n","\n","    # Parameters we optimize our classifiers on\n","    parameters = [{\"n_estimators\":[20,30,50,100,150,200]},\n","              {\"n_estimators\":[50,100,150,200,300,400,500], \"max_depth\":[3,5,6,10]},\n","              {},\n","              {\"n_estimators\":[50,100,150,200,300,400,500], \"max_depth\":[3,5,6,10,15,20]},\n","              {'kernel':['rbf','poly','sigmoid'],'gamma':['scale','auto'],'decision_function_shape':['ovo', 'ovr']},\n","              {}]\n","\n","    # Compute different measures on train and test set for every classifier\n","    clfs=[clf1,clf2,clf3,clf4,clf5,clf6]\n","    for m in range(0,6):\n","      clf_temp = GridSearchCV(clfs[m], param_grid=parameters[m], cv=5, n_jobs=1, scoring='f1_macro',).fit(x_label_train_pred,y_label_train)\n","      pred_train = clf_temp.predict(x_label_train_pred)\n","      pred_test = clf_temp.predict(x_label_test_pred)\n","      accuracy_train[i,j,m]=accuracy_score(y_label_train,pred_train)\n","      accuracy_test[i,j,m]=accuracy_score(y_label_test,pred_test)\n","      f1_train[i,j,m]=f1_score(y_label_train,pred_train,average='macro')\n","      f1_test[i,j,m]=f1_score(y_label_test,pred_test,average='macro')\n","      recall_train[i,j,m]=recall_score(y_label_train,pred_train,average='macro')\n","      recall_test[i,j,m]=recall_score(y_label_test,pred_test,average='macro')\n","      precision_train[i,j,m]=precision_score(y_label_train,pred_train,average='macro')\n","      precision_test[i,j,m]=precision_score(y_label_test,pred_test,average='macro')\n","\n","    # Save all results\n","    clfs_names=['adaboost','xgb','catboost','rfc','svc','lr']\n","    for m in range(0,6):\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/accuracy_train_'+clfs_names[m]+'.csv', accuracy_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/accuracy_test_'+clfs_names[m]+'.csv', accuracy_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/f1_train_'+clfs_names[m]+'.csv', f1_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/f1_test_'+clfs_names[m]+'.csv', f1_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/recall_train_'+clfs_names[m]+'.csv', recall_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/recall_test_'+clfs_names[m]+'.csv', recall_test[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/precision_train_'+clfs_names[m]+'.csv', precision_train[:,:,m], delimiter=',')\n","      np.savetxt('/content/drive/My Drive/Weak learners/programming/Artur/w_coordinates/precision_test_'+clfs_names[m]+'.csv', precision_test[:,:,m], delimiter=',')\n"],"execution_count":0,"outputs":[]}]}